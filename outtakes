# regex_keywords_roodujin = ["elit\S*", "konsens\S*", "undemokratisch\S*",
#                            "referend\S*", "korrupt\S*", "propagand\S*",
#                            "politiker\S*", "täusch\S*", "betrueg\S*",
#                            "betrug\S*", "\S*verrat\S*", "scham\S*", "schäm\S*",
#                            "skandal\S*", "wahrheit\S*", "unfair\S*",
#                            "unehrlich\S*", "establishm\S*", "\S*herrsch\S*",
#                            "lüge\S*"]


# # LF based on Roodujin keywords-regex search
# @labeling_function()
# def lf_keywords_roodujin_regex(x):
#     regex_roodujin = '|'.join(regex_keywords_roodujin)
#
#     # Return a label of POP if keyword in text, otherwise ABSTAIN
#     return POP if re.search(regex_roodujin, x.text, flags=re.IGNORECASE) else ABSTAIN



        # Define vectorizer
        vectorizer = TfidfVectorizer(tokenizer=self.__custom_dict_tokenizer, lowercase=True)

        # Fit vectorizer on whole corpus
        vectorizer.fit(df['wording_segments'])

        # CALCULATE TF-IDF SCORES OF ANTI_ELITE CLASSIFIED DOCS
        df_ae = df.loc[(df['POPULIST_AntiElite'] == 1) &
                       (df['POPULIST_PeopleCent'] == 0) &
                       (df['POPULIST_Sovereign'] == 0)]

        # Transform subcorpus labelled as POP
        tfidf_ae_vector = vectorizer.transform(df_ae['wording_segments']).toarray()

        # Map tf-idf scores to words in the vocab with separate column for each doc
        wordlist = pd.DataFrame({'term': vectorizer.get_feature_names()})

        for i in range(len(tfidf_ae_vector)):
            wordlist[i] = tfidf_ae_vector[i]

            # Set words as index
        wordlist.set_index('term')

        # Calculate average tf-idf over all docs
        wordlist['average_tfidf'] = wordlist.mean(axis=1)

        # Sort by average tf-idf
        wordlist.sort_values(by='average_tfidf', ascending=False, inplace=True)

        # Retrieve specified top n_words entries
        tfidf_ae_dict = wordlist[:30][['term', 'average_tfidf']]

        # Find POS tags that correspond to the keywords
        df['doc_wording_segments'] = list(self.nlp_full.pipe(df['wording_segments']))

        result_tags = {}

        # Iterate over docs in df
        for index, row in df.iterrows():
            # Iterate over tokens in doc
            for token in row.doc_wording_segments:
                # Check if token is in dict
                if token.text.lower() in tfidf_ae_dict.term.values:
                    # Check if token is already contained in result_tags dict
                    if token.text in result_tags:
                        # Check if token.pos_ is already a known pos tag for the token
                        if token.pos_ in result_tags[token.text]:
                            pass
                        # Else, append current token.pos_ as new value
                        else:
                            result_tags[token.text].append(token.pos_)

                    # Elseif token is not contained in dict, create new key,value-pair
                    else:
                        value = {token.text: [token.pos_]}
                        result_tags.update(value)



         # Generate lists of potential umlaut replacements
        replacement_list = []

        umlauts_dict = {"ä": [{"TEXT": {"REGEX": "ae"}}],
                        "Ä": [{"TEXT": {"REGEX": "Ae"}}],
                        "ö": [{"TEXT": {"REGEX": "oe"}}],
                        "Ö": [{"TEXT": {"REGEX": "Oe"}}],
                        "ü": [{"TEXT": {"REGEX": "ue"}}],
                        "Ü": [{"TEXT": {"REGEX": "Ue"}}],
                        "ß": [{"TEXT": {"REGEX": "ss"}}]}
# Replace each specified umlaut
        for key in umlauts_dict:

            replacement = key
            pattern = umlauts_dict[key]
            pattern_token = pattern[0]['TEXT']['REGEX']

            token_matcher = Matcher(self.nlp_sent.vocab)
            token_matcher.add("REPLACE", [pattern])

            key_replacement_list = []

            for index, row in df.iterrows():
                matches = token_matcher(row.Wording_doc_temp)  # print the matched results and extract out the results
                for match_id, start, end in matches:
                    # Get the string representation
                    token = row.Wording_doc_temp[start:end].text
                    token_fixed = re.sub(pattern_token, replacement, token)

                    # Add token to replacement list
                    key_replacement_list.append(token_fixed)

            replacement_list.append(key_replacement_list)




% DICT RETRIVAL

       # if head.pos_ == 'VERB':

                    # check if AUX exists
                    # Get object



            root_verbs = []
            root_nouns = []
            root_others = []

            # # Iterate over tokens to access ROOTs
            # for token in segment:
            #
            #     if token.dep_ == 'ROOT':
            #         # if ROOT is VERB
            #         if token.pos_ == 'VERB':
            #
            #             # get corresponding sb, or if available, oc
            #             root_verbs.append((token.lemma_.lower(), token.pos_, token.dep_))
            #
            #             #[child for child in token.children])
            #
            #         # if ROOT is AUXILIARY
            #         elif token.pos_ == 'AUX':
            #
            #             # get corresponding VERB
            #             childs = [child for child in token.children]
            #
            #             print(childs)
            #
            #             # get corresponding SUBJECT
            #
            #             # get corresponding OBJECT
            #
            #             # get corresponding NEGATION
            #
            #             root_verbs.append((token.lemma_.lower(), token.pos_, token.dep_))
            #
            #         # if ROOT is ADVERB
            #         elif token.pos_ == 'ADV':
            #             root_verbs.append((token.lemma_.lower(), token.pos_, token.dep_))
            #
            #         # if ROOT is NOUN
            #         elif token.pos_ in ['NOUN', 'PROPN']:
            #             root_nouns.append((token.lemma_.lower(), token.pos_, token.dep_))
            #
            #         else:
            #             root_others.append((token.lemma_.lower(), token.pos_, token.dep_))
            #             print(token.pos_)
            #             print(token.sent)
            #
            #     lemmas.append((token.lemma_.lower(), token.pos_, token.dep_))
            #     #
                # # subject would be
                # if token.dep_ in SUBJECTS or token.pos_ in ['NOUN', 'PRONOUN']:
                #     subject = token.lemma_
                #
                # # iobj for indirect object
                # if token.dep_ == "iobj":
                #     indirect_object = token.orth_
                # # dobj for direct object
                # if token.dep_ == "dobj":
                #     direct_object = token.orth_
                #
                # #print(subject, indirect_object, direct_object)
                #
                # # Extract triples of type {subj, obj, in list   "sb", "nsubj", "neg", "dobj",
                # if token.dep_ in ("ROOT", "sb"):
                #     print(token.text, token.dep_, token.head.text, token.head.pos_,
                #           [child for child in token.children])
                #
                #     triples_dict.append([(token.lemma_.lower(), token.pos_, token.dep_)])



# todo: only for debugging
                lemmas = []
                current_sent = verb.sent ##
                for token in current_sent: ##
                    lemmas.append((token.lemma_.lower(), token.pos_, token.dep_, token.head.text)) ##

___ DEP

def extract_dep_tuples(segment):
    """
    Retrieve tuples dict of type (subj, verb, verb_prefix, object, negation) for each verb in Segment
    :param segment: parsed segment
    :type segment: spacy.tokens.doc.Doc
    :return: list of triple dicts
    :rtype:  list
    """

    # Define individual components
    VERBCOMPONENTS = ['svp']
    SUBJECTS = ['sb', 'sbp']
    OBJECTS = ['oa', 'og', 'da', 'pd'] # oc,?
    NEGATIONS = ['ng']

    # Generate empty dict list
    triples_dict_list = []

    # Extract fullverb tokens
    verbs = [token for token in segment if token.pos_ in ['VERB', 'AUX']]

    # Iterate over fullverbs
    for verb in verbs:

        # todo: only to debug
        lemmas = [] ##
        current_sent = verb.sent  ##
        for token in current_sent:  ##
            lemmas.append((token.lemma_.lower(), token.pos_, token.dep_, token.head.text))  ##

        # Create empty list for components
        verb_list = []
        verb_comp_list = []
        subj_list = []
        obj_list = []
        neg_list = []

        # Extract current fullverb
        verb_list.append(verb.lemma_.lower())

        # Iterate over verb dependents
        for child in verb.children:

            # Extract separable verb prefix
            if child.dep_ in VERBCOMPONENTS:
                verb_comp_list.append(child.lemma_.lower())

            # Extract subject
            if child.dep_ in SUBJECTS:
                subj_list.append(child.lemma_.lower())

            # Extract object
            elif child.dep_ in OBJECTS:
                obj_list.append(child.lemma_.lower())

            # Extract negation
            elif child.dep_ in NEGATIONS:
                neg_list.append(child.lemma_.lower())

        # if lists are empty, return None
        if (not verb_list) & (not verb_comp_list) & (not subj_list) & (not obj_list) & (not neg_list):
            triples_dict = None

        # Else, return content
        else:
            triples_dict = {'subject': subj_list,
                            'verb': verb_list,
                            'verb_prefix': verb_comp_list,
                            'object': obj_list,
                            'negation': neg_list}

        # Generate list with dict for each verb in Segment
        triples_dict_list.append(triples_dict)

    return triples_dict_list