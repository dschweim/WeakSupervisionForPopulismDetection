# regex_keywords_roodujin = ["elit\S*", "konsens\S*", "undemokratisch\S*",
#                            "referend\S*", "korrupt\S*", "propagand\S*",
#                            "politiker\S*", "täusch\S*", "betrueg\S*",
#                            "betrug\S*", "\S*verrat\S*", "scham\S*", "schäm\S*",
#                            "skandal\S*", "wahrheit\S*", "unfair\S*",
#                            "unehrlich\S*", "establishm\S*", "\S*herrsch\S*",
#                            "lüge\S*"]


# # LF based on Roodujin keywords-regex search
# @labeling_function()
# def lf_keywords_roodujin_regex(x):
#     regex_roodujin = '|'.join(regex_keywords_roodujin)
#
#     # Return a label of POP if keyword in text, otherwise ABSTAIN
#     return POP if re.search(regex_roodujin, x.text, flags=re.IGNORECASE) else ABSTAIN



        # Define vectorizer
        vectorizer = TfidfVectorizer(tokenizer=self.__custom_dict_tokenizer, lowercase=True)

        # Fit vectorizer on whole corpus
        vectorizer.fit(df['wording_segments'])

        # CALCULATE TF-IDF SCORES OF ANTI_ELITE CLASSIFIED DOCS
        df_ae = df.loc[(df['POPULIST_AntiElite'] == 1) &
                       (df['POPULIST_PeopleCent'] == 0) &
                       (df['POPULIST_Sovereign'] == 0)]

        # Transform subcorpus labelled as POP
        tfidf_ae_vector = vectorizer.transform(df_ae['wording_segments']).toarray()

        # Map tf-idf scores to words in the vocab with separate column for each doc
        wordlist = pd.DataFrame({'term': vectorizer.get_feature_names()})

        for i in range(len(tfidf_ae_vector)):
            wordlist[i] = tfidf_ae_vector[i]

            # Set words as index
        wordlist.set_index('term')

        # Calculate average tf-idf over all docs
        wordlist['average_tfidf'] = wordlist.mean(axis=1)

        # Sort by average tf-idf
        wordlist.sort_values(by='average_tfidf', ascending=False, inplace=True)

        # Retrieve specified top n_words entries
        tfidf_ae_dict = wordlist[:30][['term', 'average_tfidf']]

        # Find POS tags that correspond to the keywords
        df['doc_wording_segments'] = list(self.nlp_full.pipe(df['wording_segments']))

        result_tags = {}

        # Iterate over docs in df
        for index, row in df.iterrows():
            # Iterate over tokens in doc
            for token in row.doc_wording_segments:
                # Check if token is in dict
                if token.text.lower() in tfidf_ae_dict.term.values:
                    # Check if token is already contained in result_tags dict
                    if token.text in result_tags:
                        # Check if token.pos_ is already a known pos tag for the token
                        if token.pos_ in result_tags[token.text]:
                            pass
                        # Else, append current token.pos_ as new value
                        else:
                            result_tags[token.text].append(token.pos_)

                    # Elseif token is not contained in dict, create new key,value-pair
                    else:
                        value = {token.text: [token.pos_]}
                        result_tags.update(value)
