{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f39d01a3-a16f-43c0-90d0-83e7aec69dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02e782f-556e-4565-a210-7171289ce4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/pfs/data5/home/ma/ma_ma/ma_dschweim/Thesis/'\n",
    "classifier = 'BERT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "001f2f6d-bf5f-44a8-9ce4-bb802a92dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "def output_and_store_endmodel_results(output_path, classifier, feature, Y_test, Y_pred, X_test, hyperparameters):\n",
    "    \"\"\"\n",
    "    Print results in console and store them in csv (merging with previous results)\n",
    "    :param output_path: path to data output\n",
    "    :type output_path: str\n",
    "    :param classifier: model used in current run\n",
    "    :type classifier: str\n",
    "    :param feature: vectorization used in current run\n",
    "    :type feature: str\n",
    "    :param Y_test: ground truth labels of test set\n",
    "    :type Y_test: pd.Series\n",
    "    :param Y_pred: predicted labels of test set\n",
    "    :type Y_pred: np.ndarray\n",
    "    :param X_test: test set\n",
    "    :type X_test: pd.DataFrame\n",
    "    :param hyperparameters: tuned hyperparameters retrieved from model\n",
    "    :type hyperparameters: dict\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "\n",
    "    print('---------------------------------------')\n",
    "    print(f\"Model: {classifier}, Feature: {feature}\")\n",
    "    print(f\"Model Test Accuracy: {accuracy_score(Y_test, Y_pred)}\")\n",
    "    print(f\"Model Test Precision: {precision_score(Y_test, Y_pred)}\")\n",
    "    print(f\"Model Test Recall: {recall_score(Y_test, Y_pred)}\")\n",
    "    print(f\"Model Test F1: {f1_score(Y_test, Y_pred, average='binary')}\")\n",
    "\n",
    "    # Save results\n",
    "    timestamp = datetime.datetime.now().strftime(\"%d-%m-%Y %H-%M-%S\")\n",
    "\n",
    "    results_df = pd.DataFrame({'model': [classifier],\n",
    "                               'vectorization': [feature],\n",
    "                               'hyperparameters': [hyperparameters],\n",
    "                               'accuracy': [accuracy_score(Y_test, Y_pred)],\n",
    "                               'precision': [precision_score(Y_test, Y_pred)],\n",
    "                               'recall': [recall_score(Y_test, Y_pred)],\n",
    "                               'f1': [f1_score(Y_test, Y_pred, average='binary')],\n",
    "                               'timestamp': [timestamp]\n",
    "                               })\n",
    "    results_df = results_df.set_index(['model', 'vectorization'])\n",
    "\n",
    "    # If results file exists, append results to file\n",
    "    if os.path.isfile(f'{output_path}\\\\Results\\\\End_Model\\\\results.csv'):\n",
    "        prev_results = pd.read_csv(f'{output_path}\\\\Results\\\\End_Model\\\\results.csv',\n",
    "                                   index_col=['model', 'vectorization'])\n",
    "\n",
    "        results_df = results_df.append(prev_results)\n",
    "\n",
    "        # only keep newest run\n",
    "        results_df = results_df[~results_df.index.duplicated()].sort_index()\n",
    "\n",
    "    results_df.to_csv(f'{output_path}\\\\Results\\\\End_Model\\\\results.csv')\n",
    "    \n",
    "    if classifier == 'BERT':\n",
    "         # Save individual predictions and corresponding information\n",
    "        model_preds = pd.DataFrame({'Content': X_test.content,\n",
    "                                    'POPULIST_PeopleCent': X_test.POPULIST_PeopleCent,\n",
    "                                    'POPULIST_AntiElite': X_test.POPULIST_AntiElite,\n",
    "                                    'POPULIST_Sovereign': X_test.POPULIST_Sovereign,\n",
    "                                    'Country': X_test.Sample_Country,\n",
    "                                    'Category': X_test.Sample_Type,\n",
    "                                    'Y_test': Y_test,\n",
    "                                    'Y_pred': Y_pred})\n",
    "\n",
    "    else:\n",
    "        # Save individual predictions and corresponding information\n",
    "        model_preds = pd.DataFrame({'Content': X_test.content,\n",
    "                                    'POPULIST_PeopleCent': X_test.POPULIST_PeopleCent,\n",
    "                                    'POPULIST_AntiElite': X_test.POPULIST_AntiElite,\n",
    "                                    'POPULIST_Sovereign': X_test.POPULIST_Sovereign,\n",
    "                                    'Country': X_test.Sample_Country,\n",
    "                                    'Category': X_test.Sample_Type,\n",
    "                                    'Y_test': Y_test.astype(int),\n",
    "                                    'Y_pred': Y_pred})\n",
    "\n",
    "    model_preds.to_csv(f'{output_path}\\\\{classifier}_{feature}_preds.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "620c7e57-17e6-41a2-8db0-d72b6e457d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2277afa3-1cf7-49f6-9f3e-6c2570f1396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_transformer(X, y, X_test, model_name):\n",
    "\n",
    "    # Instantiate Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Encode the text features for training (test data is encoded later)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Tokenize\n",
    "    X_train_tokenized = tokenizer(X_train, padding='max_length', truncation=True)\n",
    "    X_val_tokenized = tokenizer(X_val, padding='max_length', truncation=True)\n",
    "\n",
    "    # Create torch dataset\n",
    "    train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "    val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "    # Instantiate model\n",
    "    transformer_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    # Create Trainer Object\n",
    "    # Use GPU, if available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # Define Trainer\n",
    "    training_args = TrainingArguments(\"test_trainer\")\n",
    "    trainer = Trainer(\n",
    "        model=transformer_model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    # Train pre-trained model\n",
    "    transformer_model.to(device)\n",
    "    trainer.train()\n",
    "\n",
    "    # ----- 3. Predict -----#\n",
    "    # Load test data\n",
    "    X_test_tokenized = tokenizer(X_test, padding='max_length', truncation=True)\n",
    "\n",
    "    # Create torch dataset\n",
    "    test_dataset = Dataset(X_test_tokenized)\n",
    "\n",
    "    # Make prediction\n",
    "    raw_pred = trainer.predict(test_dataset)\n",
    "\n",
    "    # Preprocess raw predictions\n",
    "    y_pred = raw_pred[0].argmax(-1)\n",
    "\n",
    "    # Get best hyperparam setting\n",
    "    hyperparameters = {\n",
    "        \"batch_size\":trainer.args.per_device_train_batch_size,\n",
    "        \"learning_rate\": trainer.args.learning_rate,\n",
    "        \"num_epochs\": trainer.args.num_train_epochs}\n",
    "\n",
    "    return y_pred, hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b01fa4b0-60ab-4902-8b0d-b6ef398a9504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-german-cased/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dschweim/.cache/huggingface/transformers/98877e98ee76b3977d326fe4f54bc29f10b486c317a70b6445ac19a0603b00f0.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-german-cased/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dschweim/.cache/huggingface/transformers/0c57cb5172c1ac6c957d00597dc43c1b8b2a2cb44729a590fd0112612221f746.9a4f439638381be22bb9f116542bdaa5e1d8bb7a09a5f8ef32d9662deaf655a1\n",
      "loading file https://huggingface.co/bert-base-german-cased/resolve/main/tokenizer.json from cache at /home/ma/ma_ma/ma_dschweim/.cache/huggingface/transformers/a60c7a72be0cad1606096bd88aa22980c826a10b2482a850cfd50db5ceb3f01f.a1d3fa1580dc5318a8ad0477d679498575453bbe1ef5751aaca7fec558055f77\n",
      "loading file https://huggingface.co/bert-base-german-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-german-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-german-cased/resolve/main/tokenizer_config.json from cache at /home/ma/ma_ma/ma_dschweim/.cache/huggingface/transformers/2529d64cc99a539f2103ad09cea0d6459e181d8dc168fe06b32d25ddc68e6d3b.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-german-cased/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dschweim/.cache/huggingface/transformers/98877e98ee76b3977d326fe4f54bc29f10b486c317a70b6445ac19a0603b00f0.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-german-cased/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dschweim/.cache/huggingface/transformers/98877e98ee76b3977d326fe4f54bc29f10b486c317a70b6445ac19a0603b00f0.1f2afedb22f9784795ae3a26fe20713637c93f50e2c99101d952ea6476087e5e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-german-cased/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dschweim/.cache/huggingface/transformers/5236eea09283e87ba7c16d0571a12520ed4f076869f3d943fdbfaaa34b71e419.953a553bf3928a893b8cacf8d8c46ce6c565c095f062120aa0773821285cde25\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1988\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 747\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='747' max='747' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [747/747 03:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.039700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_trainer/checkpoint-500\n",
      "Configuration saved in test_trainer/checkpoint-500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 692\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('/pfs/data5/home/ma/ma_ma/ma_dschweim/Thesis/datasets/labeled_df_train_threshold_None.csv')\n",
    "test = pd.read_csv('/pfs/data5/home/ma/ma_ma/ma_dschweim/Thesis/datasets/labeled_df_test.csv')\n",
    "\n",
    "# Define test labels\n",
    "Y_test = test.label.tolist()\n",
    "\n",
    "# Predict test labels\n",
    "Y_pred, hyperparameters = run_transformer(X=train.content.tolist(), y= train.label.tolist(), X_test=test.content.tolist(), model_name='bert-base-german-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "450180c4-4023-4f2a-a430-5a1c5eecf755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Test Accuracy: 0.3208092485549133\n",
      "Model Test Precision: 0.3183139534883721\n",
      "Model Test Recall: 0.9954545454545455\n",
      "Model Test F1: 0.48237885462555063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Print resuls\n",
    "print(f\"Model Test Accuracy: {accuracy_score(Y_test, Y_pred)}\")\n",
    "print(f\"Model Test Precision: {precision_score(Y_test, Y_pred)}\")\n",
    "print(f\"Model Test Recall: {recall_score(Y_test, Y_pred)}\")\n",
    "print(f\"Model Test F1: {f1_score(Y_test, Y_pred, average='binary')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12e15a93-248b-4821-8d80-899fd7ff2e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Model: BERT, Feature: -\n",
      "Model Test Accuracy: 0.3208092485549133\n",
      "Model Test Precision: 0.3183139534883721\n",
      "Model Test Recall: 0.9954545454545455\n",
      "Model Test F1: 0.48237885462555063\n"
     ]
    }
   ],
   "source": [
    "output_and_store_endmodel_results(output_path=output_path, classifier=classifier, feature='-',\n",
    "                                  Y_test=Y_test, Y_pred=Y_pred, X_test=test,\n",
    "                                  hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d5aea-dbc3-472c-8ff5-b003dcddcad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
